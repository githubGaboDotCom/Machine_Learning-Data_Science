{"cells":[{"cell_type":"markdown","source":["## Questions\n\n\n__Use Chipotle July 2021__\n\n1. Return the highest bucketed dwell time for each location/month.  \n  A. Convert the bucketed dwell time to a number using the lowest value in the bucket string label. For less than 5 (<5), convert it to 2.   \n  B. Display the placekey, date_range_start, raw_visitor_counts, bucket value, visitor counts for that bucket, and a year column.   \n  C. Create a visualization that helps us understand the change in dwell times over the years.   \n2. Calculate the rolling 7-day totals using `visits_by_day` and `date_range_start`.\n  A. Your reported numbers should be correctly scaled to the state estimates.   \n  B. Use `F.posexplode()` as one way to build your calculations.   \n  D. Sort the dataframe by the 7-day totals column in descending order and display it.   \n3. Now build a new dataframe with the top store for each state and create a plot showing the rolling 7-day performance of those stores.\n  A. I did this with a `groupBy()`, `partionBy()`, and and `join()` to filter to the best placekey for each state.   \n  B. You should create a line chart that shows the rolling 1-week sum for the top store in each state.   \n4. Explain what the following `pandas_udf()` is doing in your own words.\n\n```python\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nimport pandas as pd\n####\n# descript this portion\n@pandas_udf('int', PandasUDFType.SCALAR)\ndef day_with_max_visits(series):\n  cols = [x for x in range(1,32)]\n  temp = pd.DataFrame(series.to_list(), columns=cols)\n  return temp.idxmax(axis=1)\n#####\n\ndisplay(df.withColumn('day with max', day_with_max_visits(df.visits_by_day)))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b85397bb-e944-4f18-81e1-0a157632489d"}}},{"cell_type":"code","source":["# build data in parquet format for \n## Have them read it in and then do the following\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nfrom plotnine import *\nimport pandas as pd\n\ndat = spark.read.parquet(\"dbfs:/FileStore/dat/owhekdudn.parquet\")\n\ndisplay(dat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d429d6e-92b7-446e-b7da-ad481066978e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dat.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d3d8d82-79a6-49ac-a5b5-da900c7a04d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q1: Bucket"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dab70e3c-0b06-4b95-a75d-d6543b4e3f24"}}},{"cell_type":"markdown","source":["**A. Convert the bucketed dwell time to a number using the lowest value in the bucket string label. For less than 5 (<5), convert it to 2.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63a36fea-3f69-4627-b188-c18a9d2a4d8c"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\n#Using UDF to allow my function to return STRING values as a pyspark column, and allow to work with them. It returns the key of the highest value from a dictionary.\n\n@udf(\"string\")\ndef return_highest_dwell_time(data):\n    dict_var = {}\n    dict_var['2'] = data['<5']\n    dict_var['5'] = data['5-10']\n    dict_var['11'] = data['11-20']\n    dict_var['21'] = data['21-60']\n    dict_var['61'] = data['61-120']\n    dict_var['121'] = data['121-240']\n    dict_var['240'] = data['>240']\n    \n    highest_value = max(dict_var, key=dict_var.get)\n    \n    return highest_value\n\ndat_data = dat.withColumn(\"bucket_value\", return_highest_dwell_time(dat.bucketed_dwell_times))\n\ndisplay(dat_data.select(\"bucket_value\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cfb0523-5536-40cf-8c93-c08121064607"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**B. Display the placekey, date_range_start, raw_visitor_counts, bucket value, visitor counts for that bucket, and a year column.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bf79275-4ce2-4258-9e5c-5af419cc91dd"}}},{"cell_type":"code","source":["@udf(\"integer\")\ndef return_visitor_counts(data):\n    dict_var = {}\n    dict_var['2'] = data['<5']\n    dict_var['5'] = data['5-10']\n    dict_var['11'] = data['11-20']\n    dict_var['21'] = data['21-60']\n    dict_var['61'] = data['61-120']\n    dict_var['121'] = data['121-240']\n    dict_var['240'] = data['>240']\n    \n    visitor_count = max(dict_var.values()) #Same as the prior function, it returns the maximum integer value from a dictionary of values.\n    \n    return visitor_count\n\ndat_data = dat_data.withColumn(\"visitor_counts_by_bucket\", return_visitor_counts(dat_data.bucketed_dwell_times))\n\nnew_dat_data = dat_data.select(F.col(\"placekey\"), F.col(\"date_range_start\"), F.col(\"region\"), F.col(\"visits_by_day\"), F.col(\"raw_visit_counts\"), F.col(\"bucket_value\"), F.col(\"visitor_counts_by_bucket\"), F.year(F.col(\"date_range_start\")).alias(\"Year\"), F.col(\"normalized_visits_by_state_scaling\"))\n\ndisplay(new_dat_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebff6e74-ac34-4832-9174-6cf237a11e8f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**C. Create a visualization that helps us understand the change in dwell times over the years.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a9dbcb3-70c9-4afe-8103-5d0b2978818f"}}},{"cell_type":"code","source":["pd_new_dat_data = new_dat_data.toPandas()\n\n(ggplot(pd_new_dat_data)\n + geom_col(aes(x='Year', y ='visitor_counts_by_bucket', fill = 'bucket_value'), width = 0.50)\n + theme(figure_size=(10, 6))\n + labs(x='Year', y='Visitor Counts By Bucket', title=\"Change in Dwell Times over the Years\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac2bcdf7-e18a-4350-ba17-1aaa50d02a0d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q2: Rolling window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47c52625-63c4-40e8-96b6-4c9545671752"}}},{"cell_type":"markdown","source":["**Calculate the rolling 7-day totals using visits_by_day and date_range_start.**<br>\n**A. Your reported numbers should be correctly scaled to the state estimates.**<br>\n**B. Use F.posexplode() as one way to build your calculations.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8516dcf4-2f4b-4ac3-8b3f-5eba566f9c46"}}},{"cell_type":"code","source":["#I am basically using the posexplode function to get indices of each daily visits and their values in the visits_by_day column. Then, I sum up the indices to the date_range_start column to get the specific day of the month in which we got a number of daily visits. After this, the scaling to the state estimates is done.\n\nposexploded_data = new_dat_data.select(\"*\", F.posexplode(new_dat_data.visits_by_day))\nposexploded_data = posexploded_data.withColumn(\"date\", F.date_format(F.col(\"date_range_start\"), \"yyyy-MM-dd\"))\nposexploded_data = posexploded_data.withColumn(\"month\", F.month(posexploded_data.date_range_start))\nposexploded_data = posexploded_data.withColumn(\"col\", (posexploded_data.col / posexploded_data.raw_visit_counts) * posexploded_data.normalized_visits_by_state_scaling)\nposexploded_data = posexploded_data.withColumn(\"date\", F.expr(\"date_add(date, pos)\"))\n\n#I did the rolling average by creating a window partition to average the current value and the prior 6 (a total of 7 days) sequentially. \n\nrolling7Days_window_partition = Window().rowsBetween(-6, Window.currentRow)\nposexploded_data = posexploded_data.withColumn(\"rolling_avg\", F.avg(posexploded_data.col).over(rolling7Days_window_partition))\nposexploded_data = posexploded_data.filter(posexploded_data.pos >= 6)\n   \nrolling_sum_window_partition = Window().partitionBy(\"placekey\", \"Year\", \"month\")\nrolling7_days_data = posexploded_data.withColumn(\"_7_day_total\", F.sum(posexploded_data.rolling_avg).over(rolling_sum_window_partition))\nrolling7_days_data = rolling7_days_data.filter(rolling7_days_data.pos == 7).drop(\"row_number\", \"pos\", \"col\", \"rolling_avg\", \"normalized_visits_by_state_scaling\", \"month\", \"date\")\n\ndisplay(rolling7_days_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bd3fc3f-b331-498f-9751-d8b6ae410322"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**C. Sort the dataframe by the 7-day totals column in descending order and display it.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2468df0f-e843-46c6-9b1b-00481c7d5d5e"}}},{"cell_type":"code","source":["rolling7_days_data = rolling7_days_data.sort(rolling7_days_data._7_day_total.desc())\ndisplay(rolling7_days_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"726c7415-30bc-4572-9b3b-218adb4afc73"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q3: groupby and join"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"629c53c6-db1b-450b-8800-95619764875e"}}},{"cell_type":"markdown","source":["**Now build a new dataframe with the top store for each state and create a plot showing the rolling 7-day performance of those stores.**<br>\n**A. I did this with a groupBy(), partionBy(), and and join() to filter to the best placekey for each state.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82abee15-7882-489c-a2ce-968324ffe420"}}},{"cell_type":"code","source":["#I only had to use a window partition, orderBy, and a rank function to get the top stores for each state. According to the description of question 3, the solution can also be achived by using a groupBy and a join(). I decided to use my own solution since I don't see a part where says to strictly use the groupBy and filter.\n\npartition_by_top_store = Window.partitionBy(\"region\").orderBy(F.col(\"_7_day_total\").desc())\nranked_top_store = rolling7_days_data.withColumn(\"rank\", F.rank().over(partition_by_top_store))\ntop_store_by_state = ranked_top_store.filter(F.col(\"rank\") <= 1).drop(\"row\", \"rank\")\ndisplay(top_store_by_state)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"376a6819-587c-4c47-b3ee-a127cc2b55f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**B. You should create a line chart that shows the rolling 1-week sum for the top store in each state.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fbfa27e-8a32-4d19-95d8-eb0e295fa667"}}},{"cell_type":"code","source":["#Florida and New Jersey got the biggest values of the rolling sum.\n\npd_top_store_by_state = top_store_by_state.toPandas()\n\n(ggplot(pd_top_store_by_state, aes(x='region', y='_7_day_total'))\n + geom_line(aes(group=1), color = 'blue')\n + theme(figure_size=(16, 8))\n + labs(x='Top Store by State', y='Rolling 1-Week Sum', title=\"7-day Performance of Stores\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba6c3e48-c64e-4b48-b69f-5bc7139687d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q4: Describe code\n\n**Explain what the following pandas_udf() is doing in your own words.**\n\n```python\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nimport pandas as pd\n####\n# descript this portion\n@pandas_udf('int', PandasUDFType.SCALAR)\ndef day_with_max_visits(series):\n  cols = [x for x in range(1,32)]\n  temp = pd.DataFrame(series.to_list(), columns=cols)\n  return temp.idxmax(axis=1)\n#####\n\ndisplay(df.withColumn('day with max', day_with_max_visits(df.visits_by_day)))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cb64ba6-907b-462f-aa03-2108e048dc7e"}}},{"cell_type":"markdown","source":["Explanation: This code is using a scalar pandas UDF to perform computations on a pandas series. Firstly, we have a Pyspark Dataframe called \"df\" with a visits_by_day column. We will be using the spark function \"withColumn\" to either create a new column or modify an existing one in the 'df' spark Dataframe. We have created a function called 'day_with_max_visits' to perform computations on the 'visits_by_day' column, but the issue here is that this function is expecting a pandas series parameter. We could convert the Pyspark Dataframe into a Pandas Dataframe, but that would need more code. So, instead, we can just simplify things and use a scalar pandas UDF to allow us to work with the 'visits_by_day' column as if it were a pandas series. This pandas UDF will wrap up the function mentioned and let it work with the Pyspark column as a pandas series, and it will also return a pandas series converted into a Pyspark column back to the df Dataframe. Once we input the visits_by_day values as a pandas series into our function, we have a 'cols' variable storing a sequence with the values from 1 to 31 that will be the columns for a Dataframe, and then we have the 'temp' Dataframe with 31 columns and their values converted as a list. As the last step, we return a pandas series with the index of the maximum value of the row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"546155ed-c471-4757-bf38-c9852a0f9c4c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark_challenge_takehome_student","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4467552928246421}},"nbformat":4,"nbformat_minor":0}
