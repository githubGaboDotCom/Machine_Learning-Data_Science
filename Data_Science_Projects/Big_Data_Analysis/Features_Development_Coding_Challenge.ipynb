{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport sys\nimport math\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType, FloatType\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.sql.window import Window\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e23a7dc-dd29-4f85-94d9-45bf2b6951ea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#I decided to pull the data from the ds460_sp22_muskrats feature since that table already had the ticker column included. In the table below, I am #including the necessary columns to do the wrangling and feature engineering to get my Customer Engagement score.\n###################################################################################################################################################\n\nmonthly_patterns_data = spark.sql(\"\"\"SELECT stock_symbol AS ticker, Year(date_range_start) AS year, Quarter(date_range_start) AS quarter, date_range_start, date_range_end, bucketed_dwell_times FROM ds460_sp22_muskrats.nl_feature\n\"\"\")\ndisplay(monthly_patterns_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc7c1d71-afeb-45c6-8dc0-6a92c35a1ae5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def return_score(data_collected):\n    score_CE = score_less_5_min = score_5_10_min = score_11_20_min = score_21_60_min = score_61_120_min = 0\n   #Multiply values by each dwell time to get a score reflected in the Points column. Each location will get big scores if they had visits staying longer.\n    score_less_5_min = data_collected['<5'] * 1\n    score_5_10_min = data_collected['5-10'] * 2\n    score_11_20_min = data_collected['11-20'] * 3\n    score_21_60_min = data_collected['21-60'] * 4\n    score_61_120_min = data_collected['61-120'] * 5\n    score_CE = score_less_5_min + score_5_10_min + score_11_20_min + score_21_60_min + score_61_120_min\n    return score_CE\n  \n#Generate a new column \"Points\" with the results of the return_score function.\nmonthly_patterns_data = monthly_patterns_data.withColumn(\"Points\", F.round(return_score(monthly_patterns_data.bucketed_dwell_times), 2))\n\ndisplay(monthly_patterns_data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1ed4abf-a89c-482d-bdfd-d21dc7939179"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Organizing the data a little bit and dropping duplicates. Order the rows in ascending order by year, quarter, and date_range_start. This will help me to get the zscore based on all the previous quarters.\n\nmonthly_patterns_data = monthly_patterns_data.orderBy(F.col(\"year\").asc(), F.col(\"quarter\").asc(), F.col(\"date_range_start\").asc())\nmonthly_patterns_data = monthly_patterns_data.dropDuplicates([\"ticker\", \"year\", \"quarter\", \"date_range_start\", \"date_range_end\"])\ndisplay(monthly_patterns_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b133329d-f651-4bf6-a3fa-84aca038c682"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["monthly_patterns_data = monthly_patterns_data.withColumn(\"Points\", F.col(\"Points\").cast(\"int\"))\n\n#This function returns the z-score for each row.\ndef get_z_score(value, window_partition):\n    avg = F.avg(value).over(window_partition)\n    avg_sq = F.avg(value * value).over(window_partition)\n    sd = F.sqrt(avg_sq - avg * avg)\n    return (value - avg) / sd\n\nwindow_partition = Window.rowsBetween(-sys.maxsize, 0) #This will get the data partitioned by all the previous quarters of the current row, and then create a new column to store the z-scores.\nz_scored_data = monthly_patterns_data.withColumn(\"zscore\", F.round(get_z_score(monthly_patterns_data.Points, window_partition), 4))\n\nz_scored_data = z_scored_data.na.drop() #Drop any null values if any\n\ndisplay(z_scored_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f580cbc-ffad-4d41-834f-4e8de3655110"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#The following variables and function helps to return the normal distribution value based on each z-score. I found the formula on how getting the percentage value of a z-score table. The values are returned as a list, and then I convert them into a spark dataframe. \nz_scores = []\n\nindex = 0\nz_data = z_scored_data.select('zscore').rdd.flatMap(lambda x: x).collect()\nfor x in z_data:\n    z_scores.append(.5 * (math.erf(z_data[index] / 2 ** .5) + 1))\n    index += 1\n\nzptile_data = spark.createDataFrame(z_scores, FloatType())\n\ndisplay(zptile_data) #Display the percentages in the value column\n\ndf1 = z_scored_data.withColumn(\"id1\", monotonically_increasing_id())\n\nwindow = Window.orderBy(F.col('id1'))\ndf1 = df1.withColumn('increasing_id', F.row_number().over(window))\n\ndf2 = zptile_data.withColumn(\"id2\", monotonically_increasing_id())\nwindow = Window.orderBy(F.col('id2'))\ndf2 = df2.withColumn('increasing_id2', F.row_number().over(window))\n\nnew_dataframe = df1.join(df2, df1.increasing_id == df2.increasing_id2, \"inner\").drop(\"id1\",\"id2\", \"increasing_id\", \"increasing_id2\", \"zscore\", \"bucketed_dwell_times\", \"date_range_start\", \"date_range_end\", \"Points\")\nnew_dataframe = new_dataframe.withColumnRenamed(\"value\",\"CE_Score_Percentage\")\n\ndisplay(new_dataframe) #Finally, the Customer Engagement percentages joined with the columns \"year\", \"quarter\", and \"ticker\". This is the final table that can be joined to the final table to feed our machine learning model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dcc8fcf-121a-4b3c-9e49-159756da5d53"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Order the table in descending mode based on the Customer Engagement Percentage. \nnew_dataframe = new_dataframe.orderBy(F.col(\"CE_Score_Percentage\").desc())\ndisplay(new_dataframe)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c9bed1a-d48c-4c1a-a884-e76b3c16517d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP table cgjde.sanahuano_Customer_Engagement_Score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"890936ea-8b2b-4012-a6f7-ceb093496edc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["new_dataframe.persist()\nnew_dataframe.repartition(10).write.format(\"delta\").saveAsTable(\"cgjde.sanahuano_Customer_Engagement_Score\")\nnew_dataframe.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3fcaee0-0b2a-4b95-b463-a22a8273b6bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM cgjde.sanahuano_Customer_Engagement_Score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d773fc5e-d695-4e9f-881c-cfc013563717"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDROP table model.sanahuano_Customer_Engagement_Score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b53888c7-5226-4f30-86e2-fb6d74c1a420"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_model = spark.sql(\"\"\"SELECT CE.ticker, CE.year, CE.quarter, CE.CE_Score_Percentage, T.revenue, T.eps_surprise_percentage FROM monthly_all.targets AS T INNER JOIN cgjde.sanahuano_Customer_Engagement_Score AS CE ON T.ticker = CE.ticker AND T.year = CE.year AND T.quarter_num = CE.quarter\"\"\")\n\nmy_model.persist()\nmy_model.repartition(10).write.format(\"delta\").saveAsTable(\"model.sanahuano_Customer_Engagement_Score\")\nmy_model.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1746cb9-5618-48fb-b633-95a37da2d4ef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Final Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f20ec15a-cbf4-48bd-a0f9-1a5317e7be66"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM model.sanahuano_Customer_Engagement_Score"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ab3658d-734d-4513-b984-d970f76ff25d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Customer_Engagement","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2973367396101706}},"nbformat":4,"nbformat_minor":0}
